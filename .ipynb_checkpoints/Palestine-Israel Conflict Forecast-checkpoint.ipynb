{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a900f3-b8c4-4ce8-9f28-016fb57e0867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.13.1\n",
      "Uninstalling tensorflow-2.13.1:\n",
      "  Successfully uninstalled tensorflow-2.13.1\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-macosx_10_15_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (4.22.3)\n",
      "Requirement already satisfied: setuptools in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/Arya/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Downloading tensorflow-2.13.1-cp38-cp38-macosx_10_15_x86_64.whl (216.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.13.1\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall tensorflow -y\n",
    "#!pip install tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_selection import RFECV, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ddf4d-7e39-430a-a688-c9935770730e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "data = pd.read_csv(\"data/ACLED2021-2024.csv\")\n",
    "\n",
    "\n",
    "data.drop(columns=['time_precision', 'assoc_actor_1', 'assoc_actor_2', 'iso', 'region', 'admin3', 'location', \n",
    "                  'latitude', 'longitude', 'geo_precision', 'source_scale', 'timestamp', 'tags', 'population_best', 'event_id_cnty'], \n",
    "          inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "data['event_date'] = pd.to_datetime(data['event_date'], errors = 'coerce') #changing to datetime\n",
    "#data.set_index('event_date', inplace=True)\n",
    "#data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "#data = data.dropna(subset=['event_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00346cb",
   "metadata": {},
   "source": [
    "38130 rows × 32 columns - Original Dataset Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Dupes\n",
    "initial_row_count = data.shape[0]\n",
    "data = data.drop_duplicates()\n",
    "final_row_count = data.shape[0]\n",
    "print(f\"Removed {initial_row_count - final_row_count} duplicates\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformatting / Cleaning\n",
    "\n",
    "#Addressing NA values - Categorical, NUmerical and date\n",
    "categorical_columns = ['disorder_type', 'event_type', 'sub_event_type', 'actor1', 'actor2', 'civilian_targeting', \n",
    "                       'country', 'admin1', 'admin2', 'source', 'notes']\n",
    "categorical_columns = data[categorical_columns]\n",
    "\n",
    "\n",
    "for column in categorical_columns:\n",
    "    data[column] = data[column].fillna('Not specified')\n",
    "\n",
    "numerical_columns = ['fatalities', 'inter1', 'inter2', 'interaction']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    data[column] = data[column].fillna(data[column].median()) #using median to fill\n",
    "    \n",
    "print(data.isna().sum())\n",
    "print(data.shape)\n",
    "#data.loc[:, 'actor2'] = data['actor2'].fillna('Not specified')\n",
    "#data.loc[:, 'civilian_targeting'] = data['civilian_targeting'].fillna('Not specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7282d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping Together Actor1 and 2\n",
    "def consolidate_names(name):\n",
    "    if 'Military Forces of Israel' in name:\n",
    "        return 'Military Forces of Israel'\n",
    "    elif 'Police Forces of Israel' in name:\n",
    "        return 'Police Forces of Israel'\n",
    "    elif 'Hamas Movement' in name:\n",
    "        return 'Hamas Movement'\n",
    "    elif 'Police Forces of Israel' in name or 'Government of Israel' in name:\n",
    "        return 'Government and Police Forces of Israel'\n",
    "    elif 'Police Forces of Palestine' in name or 'Government of Palestine' in name:\n",
    "        return 'Government and Police Forces of Palestine'\n",
    "    elif 'PIJ:' in name or 'Islamic Jihad' in name:\n",
    "        return 'Palestinian Islamic Jihad'\n",
    "    elif 'Hezbollah' in name:\n",
    "        return 'Hezbollah'\n",
    "    elif 'Al Aqsa' in name:\n",
    "        return 'Al Aqsa Martyrs Brigade'\n",
    "    elif 'Katibat' in name:\n",
    "        return 'Katibat Groups (Palestine)'\n",
    "    elif 'PFLP:' in name:\n",
    "        return 'Popular Front for the Liberation of Palestine'\n",
    "    elif 'DFLP:' in name:\n",
    "        return 'Democratic Front for the Liberation of Palestine'\n",
    "    elif 'Military Forces of Iran' in name:\n",
    "        return 'Iranian Revolutionary Guard Corps'\n",
    "    elif 'Islamic State' in name:\n",
    "        return 'Islamic State'\n",
    "#civilians\n",
    "    elif 'Civilians' in name:\n",
    "        if 'Israel' in name or 'Palestine' in name:\n",
    "            return name  #Keeping isr and pal civilians\n",
    "        else:\n",
    "            return 'Civilians (International)'  # grouping others as int.\n",
    "#armed groups\n",
    "    elif 'Unidentified Armed Group' in name:\n",
    "        if 'Israel' in name or 'Palestine' in name:\n",
    "            return name  \n",
    "        else:\n",
    "            return 'Unidentified Armed Group (International)'\n",
    "#military forces\n",
    "    elif 'Military Forces of' in name:\n",
    "        if 'Israel' in name or 'Palestine' in name:\n",
    "            return name  \n",
    "        else:\n",
    "            return 'Military Forces of International Forces'\n",
    "    elif 'Settlers' in name:\n",
    "        return 'Settlers (Israel)'\n",
    "    elif 'Protesters' in name or 'Rioters' in name:\n",
    "        return name  # Retains specific categories due to their distinct contexts\n",
    "    else:\n",
    "        return name #'Other Groups' \n",
    "\n",
    "# Apply the consolidation function to both actor1 and actor2\n",
    "data['actor1_grouped'] = data['actor1'].apply(consolidate_names)\n",
    "data['actor2_grouped'] = data['actor2'].apply(consolidate_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping smaller entities\n",
    "actor1_counts = data['actor1_grouped'].value_counts()\n",
    "actor2_counts = data['actor2_grouped'].value_counts()\n",
    "\n",
    "def consolidate_small_groups(name, counts): #Check if Isr or Pal if not 'name'\n",
    "    if counts[name] < 10:\n",
    "        if 'Israel' in name:\n",
    "            return 'Other (Israel)'\n",
    "        elif 'Palestine' in name:\n",
    "            return 'Other (Palestine)'\n",
    "        else:\n",
    "            return'Other Group'\n",
    "    else:\n",
    "        # Return the name if the count is 10 or more\n",
    "        return name\n",
    "\n",
    "# Apply the consolidation function to both actor1_grouped and actor2_grouped\n",
    "data['actor1_grouped'] = data['actor1_grouped'].apply(lambda x: consolidate_small_groups(x, actor1_counts))\n",
    "data['actor2_grouped'] = data['actor2_grouped'].apply(lambda x: consolidate_small_groups(x, actor2_counts))\n",
    "\n",
    "\n",
    "# Print the new value counts to confirm re-categorization\n",
    "#print(data['actor1_grouped'].value_counts())\n",
    "#print(data['actor2_grouped'].value_counts())\n",
    "\n",
    "data['actor1'] = data['actor1_grouped']\n",
    "data['actor2'] = data['actor2_grouped']\n",
    "\n",
    "data.drop(['actor1_grouped', 'actor2_grouped'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63144a-e344-4944-bf61-80400d2e220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531726a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73916dad-fec4-4d99-8fba-7cbd785c9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c6f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of zero values per column\n",
    "zero_counts = (data == 0).astype(int).sum(axis=0)\n",
    "zero_percentage = 100 * zero_counts / len(data)\n",
    "\n",
    "# Display columns with high percentages of zeros\n",
    "print(\"Percentage of zeros in each column:\")\n",
    "print(zero_percentage[zero_percentage > 0])  # Adjust the threshold as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Temporal Features for T-S\n",
    "\n",
    "#Date related\n",
    "data['year'] = data['event_date'].dt.year\n",
    "data['month'] = data['event_date'].dt.month\n",
    "data['day'] = data['event_date'].dt.day\n",
    "data['day_of_week'] = data['event_date'].dt.day_name()\n",
    "data['days_since_start'] = (data['event_date'] - data['event_date'].min()).dt.days\n",
    "\n",
    "# time since last event of the same type\n",
    "data['time_since_last_event'] = data.groupby('event_type')['event_date'].diff().dt.days\n",
    "#data['time_since_last_event'] = data.groupby('event_type').apply(lambda x: x.index.to_series().diff().dt.days).reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "# time since last disorder of the same type\n",
    "data['time_since_last_disorder'] = data.groupby('disorder_type')['event_date'].diff().dt.days\n",
    "#data['time_since_last_disorder'] = data.groupby('disorder_type').apply(lambda x: x.index.to_series().diff().dt.days).reset_index(level=0, drop=True)\n",
    "#print(data[['time_since_last_event', 'time_since_last_disorder']].head())\n",
    "\n",
    "\n",
    "# rolling avg for fatalities\n",
    "data['rolling_avg_fatalities_7d'] = data.groupby(\n",
    "    'event_type')['fatalities'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "\n",
    "# cumulative counts of events and fatalities by specific features\n",
    "data['cumulative_events'] = data.groupby(['event_type']).cumcount() + 1\n",
    "data['cumulative_fatalities'] = data.groupby(['event_type'])['fatalities'].cumsum()\n",
    "\n",
    "log_data = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transformations\n",
    "log_columns = ['fatalities', 'cumulative_events', 'cumulative_fatalities', 'rolling_avg_fatalities_7d', \n",
    "               'time_since_last_event', 'time_since_last_disorder', 'days_since_start']\n",
    "for col in log_columns:\n",
    "    log_data['log_' + col] = np.log1p(log_data[col])\n",
    "\n",
    "# Creating lagged features\n",
    "for col in log_columns:\n",
    "    log_col = 'log_' + col\n",
    "    for lag in [1, 2, 3]:\n",
    "        log_data[f'{log_col}_lag{lag}'] = log_data[log_col].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6602cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(log_data)\n",
    "#print(log_data.isna().sum())\n",
    "\n",
    "# percentage of zero values\n",
    "zero_counts = (log_data == 0).astype(int).sum(axis=0)\n",
    "zero_percentage = 100 * zero_counts / len(log_data)\n",
    "#print(zero_percentage[zero_percentage > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning log data\n",
    "\n",
    "#replacing infinities w median\n",
    "log_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "log_data.fillna(log_data.median(), inplace=True)\n",
    "\n",
    "#forward + backward prop to fill 0's \n",
    "log_data['time_since_last_event'].fillna(method='ffill', inplace=True)\n",
    "log_data['time_since_last_event'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "lag_cols = [col for col in log_data.columns if 'log_time_since_last_event' in col or 'lag' in col]\n",
    "log_data[lag_cols] = log_data[lag_cols].fillna(method='ffill').fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(log_data)\n",
    "print(log_data.isna().sum())\n",
    "\n",
    "# percentage of zero values\n",
    "zero_counts = (log_data == 0).astype(int).sum(axis=0)\n",
    "zero_percentage = 100 * zero_counts / len(log_data)\n",
    "print(zero_percentage[zero_percentage > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1884450-7fc5-4e80-8606-287c9e9bc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "\n",
    "#One-Hot for Categoricals\n",
    "categorical_cols = ['disorder_type', 'event_type', 'actor1', 'actor2', 'civilian_targeting',\n",
    "                    'country', 'admin1', 'admin2', 'day_of_week']\n",
    "log_data_encoded = pd.get_dummies(log_data, columns=categorical_cols)\n",
    "log_data_encoded\n",
    "\n",
    "#Label Encoder\n",
    "label_encoders = {}\n",
    "\n",
    "for col in ['inter1', 'inter2', 'interaction', 'sub_event_type']:\n",
    "    le = LabelEncoder()\n",
    "    log_data_encoded[col] = le.fit_transform(log_data_encoded[col])\n",
    "    label_encoders[col] = le  # storing the encoder\n",
    "#print(log_data_encoded.isna().sum())\n",
    "log_data_encoded.to_csv('data/log_data_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis for numerical cols\n",
    "numerical_cols = log_data_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_data = log_data_encoded[numerical_cols]\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "threshold = 0.85\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mututal Information features\n",
    "\n",
    "target = 'sub_event_type'\n",
    "\n",
    "# taking out the text + datetime\n",
    "features = log_data_encoded.drop(columns=[target, 'event_date', 'source', 'notes'])\n",
    "\n",
    "# actual mi score calculation\n",
    "mi_scores = mutual_info_classif(features, log_data_encoded[target], discrete_features='auto')\n",
    "\n",
    "# putting in df\n",
    "mi_df = pd.DataFrame({'Feature': features.columns, 'MI_Score': mi_scores})\n",
    "mi_df.sort_values('MI_Score', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='MI_Score', y='Feature', data=mi_df.sort_values('MI_Score', ascending=False).head(20))\n",
    "plt.title('Top 20 Features by Mutual Information')\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f294020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Importance\n",
    "#numeric_encoded_log_data\n",
    "#Dropping numeric + T/T Splitting\n",
    "X = log_data_encoded.drop(['sub_event_type',  'event_date', 'source', 'notes'], axis=1)\n",
    "y = log_data_encoded['sub_event_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\n",
    "\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# feature importances into df\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "rf_df = pd.DataFrame({'Feature': X_train.columns, 'RF_Importance': importances})\n",
    "rf_df.sort_values('RF_Importance', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='RF_Importance', y='Feature', data=rf_df.sort_values('RF_Importance', ascending=False).head(20))\n",
    "plt.title('Top 20 Features by Random Forest Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mi_df.shape)\n",
    "print(rf_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8746ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c42810",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame({'Feature': X_train.columns,\n",
    "                                    'RF_Importance': forest.feature_importances_})\n",
    "\n",
    "#merging datasets\n",
    "combined_importances = pd.merge(mi_df, rf_df, on='Feature', how='outer')\n",
    "\n",
    "combined_importances.sort_values(by='MI_Score', ascending=False, inplace=True)\n",
    "\n",
    "combined_importances = pd.merge(mi_df, rf_df, on='Feature', how='outer')\n",
    "\n",
    "#set the amount of feaatures\n",
    "sorted_idx = combined_importances.sort_values(by='MI_Score', ascending=False)['Feature'].head(30)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='MI_Score', y='Feature', data=combined_importances[combined_importances['Feature'].isin(sorted_idx)], \n",
    "            color='blue', label='MI Score')\n",
    "sns.barplot(x='RF_Importance', y='Feature', data=combined_importances[combined_importances['Feature'].isin(sorted_idx)], \n",
    "            color='red', alpha=0.6, label='Random Forest Importance')\n",
    "plt.title('Comparison of Feature Importance by MI and Random Forest')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting arbitrary threshold to see the \"small\" values\n",
    "test_thresh1 = 0.2\n",
    "test_thresh2 = 0.01\n",
    "\n",
    "#mi scores\n",
    "low_mi_features = mi_df[mi_df['MI_Score'] <= test_thresh1]\n",
    "\n",
    "#rf importances\n",
    "low_rf_features = rf_df[rf_df['RF_Importance'] <= test_thresh2]\n",
    "\n",
    "# intersection of low importance features from both\n",
    "test_features = pd.merge(low_mi_features, low_rf_features, on='Feature', how='inner')\n",
    "\n",
    "print(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI score\n",
    "mi_percentile = 50  # aka keeping top 50%\n",
    "mi_threshold = np.percentile(mi_df['MI_Score'], mi_percentile)\n",
    "#top_mi_features = mi_df[mi_df['MI_Score'] >= mi_threshold]\n",
    "selected_mi_features = mi_df[mi_df['MI_Score'] >= mi_threshold]['Feature'].tolist()\n",
    "#top_mi_features.shape\n",
    "#print(selected_mi_features)\n",
    "\n",
    "# random forest importance\n",
    "rf_percentile = 50  # aka keeping top 50% | top 30% is 43 features\n",
    "rf_threshold = np.percentile(rf_df['RF_Importance'], rf_percentile)\n",
    "#top_rf_features = rf_df[rf_df['RF_Importance'] >= rf_threshold]\n",
    "selected_rf_features = rf_df[rf_df['RF_Importance'] >= \n",
    "                                      rf_threshold]['Feature'].tolist()\n",
    "#top_rf_features.shape\n",
    "#print(selected_rf_features)\n",
    "\n",
    "#Combining into 1\n",
    "selected_features = list(set(selected_mi_features) & set(selected_rf_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_features_combined = pd.merge(top_mi_features, top_rf_features, on='Feature', how='inner')\n",
    "#print(top_features_combined)\n",
    "#top_features_combined.to_csv('data/top_features_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92185467",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected Features\n",
    "X_selected = log_data_encoded[selected_features]\n",
    "y = log_data_encoded['sub_event_type']\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42\n",
    "                                                                      , shuffle=False)\n",
    "\n",
    "#REgular model minus text\n",
    "X = log_data_encoded.drop(['sub_event_type', 'event_date', 'source', 'notes'], axis=1) # 'event_date', \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95af615",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_forecast_last(train_data):\n",
    "    return train_data.iloc[-1]\n",
    "\n",
    "last_value = naive_forecast_last(y_train)  # Get the last value from the training target\n",
    "predictions = [last_value] * len(y_test)  # Create a list of predictions for the test set\n",
    "\n",
    "# Evaluate the prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of Naïve Forecast: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_forecast_last(train_data):\n",
    "    return train_data.iloc[-1]\n",
    "\n",
    "last_value = naive_forecast_last(y_train)  # Get the last value from the training target\n",
    "predictions = [last_value] * len(y_test)  # Create a list of predictions for the test set\n",
    "\n",
    "# Evaluate the prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of Naïve Forecast: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Selected \n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "tree_classifier.fit(X_selected_train, y_train)\n",
    "\n",
    "tree_predictions = tree_classifier.predict(X_selected_test)\n",
    "\n",
    "tree_accuracy = accuracy_score(y_test, tree_predictions)\n",
    "tree_classification_report = classification_report(y_test, tree_predictions)\n",
    "print(f\"Accuracy of Decision Tree Selected: {tree_accuracy:.2f}\")\n",
    "print(tree_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c235818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Baseline\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "tree_accuracy = accuracy_score(y_test, tree_predictions)\n",
    "tree_classification_report = classification_report(y_test, tree_predictions)\n",
    "print(f\"Accuracy of Decision Tree Selected: {tree_accuracy:.2f}\")\n",
    "print(tree_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7498bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Selected\n",
    "\n",
    "logistic_regressor = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "logistic_regressor.fit(X_train, y_train)\n",
    "\n",
    "logistic_predictions = logistic_regressor.predict(X_test)\n",
    "\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "logistic_classification_report = classification_report(y_test, logistic_predictions)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression Baseline: {logistic_accuracy:.2f}\")\n",
    "print(logistic_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Baseline\n",
    "\n",
    "logistic_regressor = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "logistic_regressor.fit(X_selected_train, y_train)\n",
    "\n",
    "logistic_predictions = logistic_regressor.predict(X_selected_test)\n",
    "\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "logistic_classification_report = classification_report(y_test, logistic_predictions)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression Baseline: {logistic_accuracy:.2f}\")\n",
    "print(logistic_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_log_data = log_data_encoded.drop(columns=[target,'source', 'notes'])\n",
    "\n",
    "def test_stationarity(series):\n",
    "    result = adfuller(series.dropna(), autolag='AIC') \n",
    "    return {\"Test Statistic\": result[0], \"p-value\": result[1], \"Critical Values\": result[4]}\n",
    "\n",
    "results = {column: test_stationarity(numeric_log_data[column]) for column in numeric_log_data.columns}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differencing to 'year', 'month', and 'days_since_start'\n",
    "numeric_log_data['year_diff'] = numeric_log_data['year'].diff()\n",
    "numeric_log_data['month_diff'] = numeric_log_data['month'].diff()\n",
    "numeric_log_data['days_since_start_diff'] = numeric_log_data['days_since_start'].diff()\n",
    "\n",
    "#data = data.dropna(subset=['year_diff', 'month_diff', 'days_since_start_diff'])\n",
    "\n",
    "# testing stationarity after differencing\n",
    "new = {\n",
    "    'year_diff': test_stationarity(numeric_log_data['year_diff']),\n",
    "    'month_diff': test_stationarity(numeric_log_data['month_diff']),\n",
    "    'days_since_start_diff': test_stationarity(numeric_log_data['days_since_start_diff'])\n",
    "}\n",
    "\n",
    "new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e9593",
   "metadata": {},
   "source": [
    "Everything is now stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2807d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b53138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to categorical\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "#  LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(np.unique(y)), output_dim=100, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "#eval\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06e477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a016f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
